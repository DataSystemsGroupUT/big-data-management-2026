{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# Week 02 - Spark Batch Processing with DataFrames\n",
    "\n",
    "**Topics covered:**\n",
    "1. Creating DataFrames (5 methods)\n",
    "2. Writing DataFrames\n",
    "3. Column transformations\n",
    "4. Row transformations\n",
    "5. Aggregations\n",
    "6. Datetime functions\n",
    "7. Custom schemas\n",
    "8. Efficient Writes\n",
    "   - Partitioned Writes\n",
    "   - Write Modes and Idempotency\n",
    "9. Complex types: Struct & Array\n",
    "10. Complex types: Map\n",
    "11. Pivot tables\n",
    "12. Window functions\n",
    "13. Joins\n",
    "    - Reading the Query Plan\n",
    "14. User-Defined Functions (UDFs)\n",
    "15. Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s0",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. SparkSession Setup\n",
    "\n",
    "`SparkContext` is the low-level entry point for RDD operations.  \n",
    "`SparkSession` is the unified entry point for DataFrame and SQL operations, and wraps the SparkContext internally.  \n",
    "Enabling Hive support lets Spark persist tables to a local Hive metastore (useful for `saveAsTable` and `spark.table()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s0-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "sc = SparkContext('local', 'week02_practice')\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('week02_practice')\n",
    "    .enableHiveSupport()   # persist tables to local Hive metastore\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Creating DataFrames\n",
    "\n",
    "There are five common ways to create a DataFrame in PySpark.  \n",
    "In practice, reading from a file source (CSV, JSON, Parquet) is most common for batch pipelines.  \n",
    "Reading from tables or SQL is useful when working in a shared catalog environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s1-rdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: from an RDD\n",
    "first_rdd = sc.parallelize([\n",
    "    (1, \"Batman\"),\n",
    "    (2, \"Superman\"),\n",
    "    (3, \"Spiderman\")\n",
    "])\n",
    "\n",
    "first_df = spark.createDataFrame(first_rdd, [\"id\", \"name\"])\n",
    "first_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s1-file",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: from a file source (CSV, JSON, Parquet)\n",
    "# CSV - specify delimiter, header, and schema inference\n",
    "crimes_df = (\n",
    "    spark.read\n",
    "    .option(\"sep\", \"\\t\")       # tab-separated\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"data/input/Chicago-Crimes-2018.csv\")\n",
    ")\n",
    "\n",
    "# JSON - Spark handles nested structures automatically\n",
    "events_df = (\n",
    "    spark.read\n",
    "    .option(\"inferSchema\", True)\n",
    "    .json(\"data/input/events-500k.json\")\n",
    ")\n",
    "\n",
    "# Parquet - schema is embedded in the file, no options needed\n",
    "sales_df = spark.read.parquet(\"data/input/sales.parquet\")\n",
    "users_df = spark.read.parquet(\"data/input/users.parquet\")\n",
    "\n",
    "events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s1-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: from a catalog table\n",
    "# First we need to write a table (see Section 2), then read it back\n",
    "events_df.write.mode(\"overwrite\").saveAsTable(\"event_table\")\n",
    "\n",
    "events_table_df = spark.table(\"event_table\")\n",
    "events_table_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s1-sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: from a SQL statement\n",
    "# Any table registered in the catalog can be queried directly\n",
    "events_sql_df = spark.sql(\"\"\"\n",
    "    SELECT device, event_name, CURRENT_DATE() AS cdate\n",
    "    FROM event_table\n",
    "    LIMIT 50\n",
    "\"\"\")\n",
    "\n",
    "events_sql_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s1-row",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 5: from Row objects\n",
    "# Row is Spark's generic record type; columns are assigned by keyword argument\n",
    "from pyspark.sql import Row\n",
    "\n",
    "my_data = Row(\"id\", \"product\", \"cost\")   # define a row template\n",
    "\n",
    "rows_df = spark.createDataFrame([\n",
    "    my_data(1, \"mac\",     1000),\n",
    "    my_data(2, \"windows\",  500),\n",
    "    my_data(3, \"linux\",    700)\n",
    "])\n",
    "\n",
    "rows_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s2",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Writing DataFrames\n",
    "\n",
    "DataFrames can be persisted to files or to a catalog table.  \n",
    "Parquet is the recommended columnar format for Spark batch pipelines - it supports efficient compression and predicate pushdown.  \n",
    "Temporary views are session-scoped and are useful for SQL queries without touching disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s2-parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet with Snappy compression\n",
    "# mode options: \"overwrite\", \"append\", \"error\" (default), \"ignore\"\n",
    "(\n",
    "    first_df.write\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"data/output/first.parquet\")\n",
    ")\n",
    "\n",
    "# Verify by reading back\n",
    "spark.read.parquet(\"data/output/first.parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s2-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a managed Hive table - persists to disk across sessions\n",
    "# Use spark.catalog.listTables() to see registered tables\n",
    "(\n",
    "    events_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"event_table\")\n",
    ")\n",
    "\n",
    "print(\"Tables in catalog:\", [t.name for t in spark.catalog.listTables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s2-tempview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary SQL view - session-scoped, not written to disk\n",
    "events_df.createOrReplaceTempView(\"events_view\")\n",
    "\n",
    "spark.sql(\"SELECT event_name, COUNT(*) as n FROM events_view GROUP BY event_name ORDER BY n DESC\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Column Transformations\n",
    "\n",
    "Column transformations produce a new DataFrame - Spark DataFrames are immutable.  \n",
    "There are multiple equivalent ways to reference a column: dot notation, `F.col()`, or string indexing.  \n",
    "`.withColumn()` is the idiomatic way to add or replace a column without rewriting the whole select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s3-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns - multiple equivalent styles\n",
    "events_df.select(\"user_id\", \"device\").show(3)\n",
    "\n",
    "# F.col() allows chaining methods on the column object\n",
    "events_df.select(F.col(\"user_id\"), F.col(\"device\")).show(3)\n",
    "\n",
    "# Wildcard: select all top-level columns\n",
    "events_df.select(F.col(\"*\")).show(3)\n",
    "\n",
    "# Nested struct fields via dot notation in the select string\n",
    "events_df.select(\"user_id\", \"geo.city\", \"geo.state\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s3-selectexpr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectExpr allows inline SQL expressions (CASE, IN, arithmetic, etc.)\n",
    "events_df.selectExpr(\n",
    "    \"user_id\",\n",
    "    \"device IN ('macOS', 'iOS') AS apple_user\",\n",
    "    \"CASE WHEN device = 'Windows' THEN 'Microsoft' ELSE 'Other' END AS platform\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s3-drop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop multiple columns at once\n",
    "anonymous_df = events_df.drop(\"user_id\", \"geo\", \"device\")\n",
    "anonymous_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s3-withcolumn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn - add a new column or replace an existing one\n",
    "mobile_df = events_df.withColumn(\"mobile\", F.col(\"device\").isin(\"iOS\", \"Android\"))\n",
    "mobile_df.select(\"device\", \"mobile\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s3-rename",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column\n",
    "location_df = events_df.withColumnRenamed(\"geo\", \"location\")\n",
    "location_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s3-when",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional column values with when / otherwise\n",
    "# Equivalent to a CASE WHEN statement in SQL\n",
    "warranty_df = events_df.select(\n",
    "    \"*\",\n",
    "    F.when(F.col(\"event_name\") == \"warranty\", \"issue\")\n",
    "     .when(F.col(\"event_name\") == \"cart\", \"sale\")\n",
    "     .otherwise(\"other\")\n",
    "     .alias(\"event_class\")\n",
    ")\n",
    "\n",
    "warranty_df.select(\"event_name\", \"event_class\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s4",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Row Transformations\n",
    "\n",
    "Row transformations reduce or reorder the rows in a DataFrame without changing its schema.  \n",
    "Filtering pushes conditions down to the scan layer, making it one of the most impactful optimisations.  \n",
    "`dropDuplicates(subset)` is more useful than `distinct()` when you want uniqueness on a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s4-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using a SQL string expression\n",
    "purchases_df = events_df.filter(\"ecommerce.total_item_quantity > 0\")\n",
    "purchases_df.show(3)\n",
    "\n",
    "# Filter using column objects - allows combining multiple conditions\n",
    "revenue_df = events_df.filter(\n",
    "    (F.col(\"ecommerce.purchase_revenue_in_usd\").isNotNull()) &\n",
    "    (F.col(\"ecommerce.total_item_quantity\") > 1)\n",
    ")\n",
    "revenue_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s4-distinct",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct() removes fully duplicate rows\n",
    "# dropDuplicates(subset) keeps the first occurrence per unique value combination\n",
    "distinct_event_names_df = events_df.dropDuplicates([\"event_name\"])\n",
    "distinct_event_names_df.select(\"event_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s4-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to the first n rows\n",
    "events_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s4-sort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ascending (default)\n",
    "events_df.sort(\"event_timestamp\").show(3)\n",
    "\n",
    "# Sort descending using .desc()\n",
    "events_df.sort(F.col(\"event_timestamp\").desc()).show(3)\n",
    "\n",
    "# Multi-column sort - orderBy is an alias for sort\n",
    "events_df.orderBy([\"device\", \"event_timestamp\"]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s5",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Aggregations\n",
    "\n",
    "`groupBy()` returns a `GroupedData` object; you chain an aggregation function to get a new DataFrame.  \n",
    "The `.agg()` method lets you compute multiple aggregations in a single pass over the data.  \n",
    "Use `F.approx_count_distinct()` instead of `F.countDistinct()` for large datasets - it is much faster with minimal error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s5-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count per group\n",
    "events_df.groupBy(\"event_name\").count().orderBy(F.desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s5-avg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average - column name uses the original column name by default\n",
    "events_df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s5-sum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum with multiple grouping keys\n",
    "events_df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s5-agg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg() - multiple aggregations in one step, with aliases\n",
    "state_agg_df = events_df.groupBy(\"geo.state\").agg(\n",
    "    F.sum(\"ecommerce.total_item_quantity\").alias(\"total_items\"),\n",
    "    F.avg(\"ecommerce.purchase_revenue_in_usd\").alias(\"avg_revenue\"),\n",
    "    F.approx_count_distinct(\"user_id\").alias(\"distinct_users\")\n",
    ")\n",
    "\n",
    "state_agg_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s6",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Datetime Functions\n",
    "\n",
    "Spark timestamps are stored as `TimestampType` (microseconds since epoch internally).  \n",
    "The events dataset stores `event_timestamp` as a long integer in microseconds - divide by 1e6 before casting.  \n",
    "Always use Spark's built-in datetime functions (`F.date_format`, `F.year`, etc.) rather than Python's `datetime`, which would require a UDF and break the Catalyst optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s6-cast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast epoch microseconds to a proper timestamp\n",
    "timestamp_df = events_df.withColumn(\n",
    "    \"event_ts\",\n",
    "    (F.col(\"event_timestamp\") / 1e6).cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "timestamp_df.select(\"event_timestamp\", \"event_ts\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s6-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format a timestamp to a human-readable string\n",
    "# Pattern follows Java SimpleDateFormat: MMMM = full month name, HH = 24h hour\n",
    "format_df = (\n",
    "    timestamp_df\n",
    "    .withColumn(\"date_str\", F.date_format(\"event_ts\", \"MMMM dd, yyyy\"))\n",
    "    .withColumn(\"time_str\", F.date_format(\"event_ts\", \"HH:mm:ss\"))\n",
    ")\n",
    "\n",
    "format_df.select(\"event_ts\", \"date_str\", \"time_str\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s6-extract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract individual date/time parts\n",
    "datetime_df = (\n",
    "    timestamp_df\n",
    "    .withColumn(\"year\",      F.year(\"event_ts\"))\n",
    "    .withColumn(\"month\",     F.month(\"event_ts\"))\n",
    "    .withColumn(\"dayofweek\", F.dayofweek(\"event_ts\"))  # 1 = Sunday\n",
    "    .withColumn(\"hour\",      F.hour(\"event_ts\"))\n",
    "    .withColumn(\"minute\",    F.minute(\"event_ts\"))\n",
    ")\n",
    "\n",
    "datetime_df.select(\"event_ts\", \"year\", \"month\", \"dayofweek\", \"hour\", \"minute\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s6-todate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_date() truncates to just the date part\n",
    "date_df = timestamp_df.withColumn(\"date\", F.to_date(\"event_ts\"))\n",
    "date_df.select(\"event_ts\", \"date\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s6-add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_add() / date_sub() for calendar arithmetic\n",
    "# SQL interval syntax also works: selectExpr(\"event_ts + interval 2 days\")\n",
    "plus_df = timestamp_df.withColumn(\"plus_two_days\", F.date_add(\"event_ts\", 2))\n",
    "plus_df.select(\"event_ts\", \"plus_two_days\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s7",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Custom Schemas\n",
    "\n",
    "By default Spark *infers* the schema by scanning the data, which is expensive for large files.  \n",
    "Providing an explicit schema makes reads faster and gives you full control over column names and types.  \n",
    "Schema inference also maps JSON objects to `MapType`, which is harder to work with than `StructType` - as we'll see in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s7-fetch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a sample JSON dataset (airlines delay statistics)\n",
    "import requests\n",
    "\n",
    "r = requests.get(\"https://corgis-edu.github.io/corgis/datasets/json/airlines/airlines.json\")\n",
    "print(r.json()[:1])   # preview first record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s7-infer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferred schema: Python dicts become MapType columns - hard to query\n",
    "airlines_map_df = spark.createDataFrame(r.json())\n",
    "airlines_map_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s7-method1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: StructType + StructField objects\n",
    "# StructField(name, dataType, nullable)\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType\n",
    ")\n",
    "\n",
    "airport_schema = StructType([\n",
    "    StructField(\"Airport\", StructType([\n",
    "        StructField(\"Code\", StringType(), True),\n",
    "        StructField(\"Name\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"Statistics\", StructType([\n",
    "        StructField(\"Carriers\", StructType([\n",
    "            StructField(\"Names\", StringType(), True),\n",
    "            StructField(\"Total\", IntegerType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"Flights\", StructType([\n",
    "            StructField(\"Delayed\",   LongType(), True),\n",
    "            StructField(\"Cancelled\", LongType(), True),\n",
    "            StructField(\"On Time\",   LongType(), True),\n",
    "            StructField(\"Total\",     LongType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"Time\", StructType([\n",
    "        StructField(\"Label\",      StringType(),  True),\n",
    "        StructField(\"Month\",      IntegerType(), True),\n",
    "        StructField(\"Year\",       IntegerType(), True),\n",
    "        StructField(\"Month Name\", StringType(),  True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "airlines_df = spark.createDataFrame(r.json(), schema=airport_schema)\n",
    "airlines_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s7-method2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: StructType .add() chaining - more readable for large schemas\n",
    "airport_add_schema = (\n",
    "    StructType()\n",
    "    .add(\"Airport\", StructType()\n",
    "         .add(\"Code\", StringType())\n",
    "         .add(\"Name\", StringType()))\n",
    "    .add(\"Time\", StructType()\n",
    "         .add(\"Month\", IntegerType())\n",
    "         .add(\"Year\",  IntegerType()))\n",
    ")\n",
    "\n",
    "airlines_add_df = spark.createDataFrame(r.json(), schema=airport_add_schema)\n",
    "airlines_add_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s7-method3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: DDL string - concise, good for simple schemas\n",
    "# Columns absent from the data will appear as null\n",
    "airport_string_schema = \"Airport STRUCT<Code: STRING, Name: STRING>, Time STRUCT<Month: INTEGER, Year: INTEGER>\"\n",
    "\n",
    "airlines_str_df = spark.createDataFrame(r.json(), schema=airport_string_schema)\n",
    "airlines_str_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s7-struct-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a StructType schema, nested fields are easy to access with dot notation\n",
    "airlines_df.select(\n",
    "    \"Airport.Code\",\n",
    "    \"Airport.Name\",\n",
    "    \"Time.Year\",\n",
    "    \"Time.Month Name\",\n",
    "    \"Statistics.Flights.Delayed\",\n",
    "    \"Statistics.Flights.Cancelled\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-partitioned-writes",
   "metadata": {},
   "source": [
    "## 8. Efficient Writes\n",
    "\n",
    "### Partitioned Writes\n",
    "When you write with `.partitionBy()`, Spark creates a folder hierarchy on disk (e.g. `year=2003/month=6/`).  \n",
    "This is how batch tables are organised so downstream queries can skip irrelevant folders - a technique called *partition pruning*.  \n",
    "The partition key should match the most common filter column; writing is slightly slower (sort + separate files per partition), but reads become dramatically faster when filtered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-partitioned-flat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the nested airlines_df into a wide table before partitioning\n",
    "airlines_flat_df = airlines_df.select(\n",
    "    F.col(\"Airport.Code\").alias(\"airport_code\"),\n",
    "    F.col(\"Airport.Name\").alias(\"airport_name\"),\n",
    "    F.col(\"Time.Year\").alias(\"year\"),\n",
    "    F.col(\"Time.Month\").alias(\"month\"),\n",
    "    F.col(\"Statistics.Flights.Delayed\").alias(\"flights_delayed\"),\n",
    "    F.col(\"Statistics.Flights.Cancelled\").alias(\"flights_cancelled\"),\n",
    "    F.col(\"Statistics.Flights.Total\").alias(\"flights_total\")\n",
    ")\n",
    "\n",
    "airlines_flat_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"data/output/airlines_partitioned.parquet\")\n",
    "\n",
    "airlines_flat_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-partitioned-ls",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sub-folder is a partition - Spark reads only the folders matching your WHERE clause\n",
    "!find data/output/airlines_partitioned.parquet -type d | head -20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-partitioned-read",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back with a filter - look for PartitionFilters in the scan node\n",
    "# Spark skips all folders except year=2006/month=1\n",
    "spark.read.parquet(\"data/output/airlines_partitioned.parquet\") \\\n",
    "    .filter(\"year = 2006 AND month = 1\") \\\n",
    "    .explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266606c-a610-4acd-ad58-31dd4a529b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back with a filter - look for PartitionFilters in the scan node\n",
    "# Spark skips all folders except year=2006/month=1\n",
    "airlines_flat_df \\\n",
    "    .filter(\"year = 2006 AND month = 1\") \\\n",
    "    .explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-write-modes",
   "metadata": {},
   "source": [
    "### Write Modes and Idempotency\n",
    "\n",
    "If your job fails and you rerun it, what happens?  \n",
    "With `append` mode you get duplicate data; with `overwrite` mode the second run replaces the first - same input produces the same output.  \n",
    "This property is called *idempotency*: safe to rerun without side-effects. In production batch pipelines, prefer `overwrite` on a specific partition over `append`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-write-modes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danger: append mode creates duplicates on rerun\n",
    "small_df = airlines_flat_df.filter(\"year = 2003 AND month = 6\")\n",
    "\n",
    "small_df.write.mode(\"append\").parquet(\"data/output/append_demo\")\n",
    "small_df.write.mode(\"append\").parquet(\"data/output/append_demo\")\n",
    "print(\"Append twice, row count:\", spark.read.parquet(\"data/output/append_demo\").count())\n",
    "# Expected: 2x the original count\n",
    "\n",
    "# Safe: overwrite mode is idempotent\n",
    "small_df.write.mode(\"overwrite\").parquet(\"data/output/overwrite_demo\")\n",
    "small_df.write.mode(\"overwrite\").parquet(\"data/output/overwrite_demo\")\n",
    "print(\"Overwrite twice, row count:\", spark.read.parquet(\"data/output/overwrite_demo\").count())\n",
    "# Expected: same as original count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s8",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Complex Types: Struct & Array\n",
    "\n",
    "Spark natively handles nested data types: `StructType` (record with named fields), `ArrayType` (ordered list), and `MapType` (key-value pairs).  \n",
    "`explode()` turns each array element into its own row, which is essential for flattening nested datasets.  \n",
    "`explode_outer()` preserves rows where the array is null or empty (equivalent to a LEFT JOIN LATERAL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s8-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sales dataset has an 'items' array column\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s8-explode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode() - one row per array element; rows with null/empty array are dropped\n",
    "# explode_outer() - same but keeps rows where array is null\n",
    "details_df = (\n",
    "    sales_df\n",
    "    .withColumn(\"items\", F.explode(\"items\"))           # flatten the items array\n",
    "    .select(\"email\", \"items.item_name\")\n",
    "    .withColumn(\"details\", F.split(F.col(\"item_name\"), \" \"))  # split string → array\n",
    ")\n",
    "\n",
    "details_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s8-array-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_contains() - boolean check\n",
    "# element_at()    - index access (1-based!)\n",
    "mattress_df = (\n",
    "    details_df\n",
    "    .filter(F.array_contains(F.col(\"details\"), \"Mattress\"))\n",
    "    .withColumn(\"size\",    F.element_at(F.col(\"details\"), 2))\n",
    "    .withColumn(\"quality\", F.element_at(F.col(\"details\"), 1))\n",
    ")\n",
    "\n",
    "pillow_df = (\n",
    "    details_df\n",
    "    .filter(F.array_contains(F.col(\"details\"), \"Pillow\"))\n",
    "    .withColumn(\"size\",    F.element_at(F.col(\"details\"), 1))\n",
    "    .withColumn(\"quality\", F.element_at(F.col(\"details\"), 2))\n",
    ")\n",
    "\n",
    "mattress_df.show(3)\n",
    "pillow_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s8-unionbyname",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unionByName() aligns columns by name, not by position\n",
    "# Safer than union() when schemas might have columns in different orders\n",
    "union_df = mattress_df.unionByName(pillow_df).drop(\"details\")\n",
    "union_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s8-collectset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect_set() - aggregation that returns an array of distinct values per group\n",
    "options_df = (\n",
    "    union_df.groupBy(\"email\")\n",
    "    .agg(\n",
    "        F.collect_set(\"size\").alias(\"size_options\"),\n",
    "        F.collect_set(\"quality\").alias(\"quality_options\")\n",
    "    )\n",
    ")\n",
    "\n",
    "options_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s9",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Complex Types: Map\n",
    "\n",
    "`MapType` columns store key-value pairs (like Python dicts).  \n",
    "They appear when Spark infers the schema of JSON objects without an explicit schema provided - which is exactly what we saw in section 7.  \n",
    "In production, prefer converting maps to structs via a custom schema for type safety and easier access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s9-map",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inferred schema from the airlines JSON produces MapType columns\n",
    "airlines_map_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s9-map-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three ways to extract a value from a MapType column\n",
    "airlines_map_df.select(\n",
    "    F.col(\"Airport\").getItem(\"Code\").alias(\"code_getItem\"),  # method 1: .getItem()\n",
    "    F.col(\"Airport\")[\"Code\"].alias(\"code_bracket\"),           # method 2: bracket notation\n",
    "    F.col(\"Airport.Code\").alias(\"code_dot\"),                  # method 3: dot notation\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s9-map-keys-vals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_keys() and map_values() return all keys/values as arrays\n",
    "airlines_map_df.select(\n",
    "    F.map_keys(\"Airport\").alias(\"keys\"),\n",
    "    F.map_values(\"Airport\").alias(\"values\")\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s10",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Pivot Tables\n",
    "\n",
    "Pivot reshapes data from a *long* format (one row per observation) to a *wide* format (one column per category value).  \n",
    "The syntax is: `.groupBy(row_keys).pivot(column_key).agg(value)`.  \n",
    "Pivot can be expensive on high-cardinality columns - pass a list of known values to the `pivot()` call to limit the number of output columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s10-airbnb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airbnb Amsterdam listings - average price by neighbourhood and number of guests\n",
    "airbnb_df = spark.read.parquet(\"data/input/amsterdam-listings-2018-12-06.parquet\")\n",
    "airbnb_df.select(\"city\", \"neighbourhood\", \"accommodates\", \"price\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s10-airbnb-pivot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot: rows = neighbourhood, columns = accommodates count, values = mean price\n",
    "(\n",
    "    airbnb_df\n",
    "    .select(\"city\", \"neighbourhood\", \"accommodates\", \"price\")\n",
    "    .groupBy(\"city\", \"neighbourhood\")\n",
    "    .pivot(\"accommodates\")          # one column per distinct value\n",
    "    .mean(\"price\")\n",
    "    .na.fill(0)\n",
    "    .orderBy(F.desc(\"2\"))           # order by the '2 guests' column\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s10-wiki",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia pageviews - total requests by date and hour, split by site\n",
    "wiki_df = spark.read.parquet(\"data/input/pageviews_by_second.parquet\")\n",
    "wiki_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s10-wiki-pivot",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    wiki_df\n",
    "    .selectExpr(\n",
    "        \"cast(timestamp as date) AS date\",\n",
    "        \"hour(timestamp) AS hour\",\n",
    "        \"site\",\n",
    "        \"requests\"\n",
    "    )\n",
    "    .groupBy(\"date\", \"hour\")\n",
    "    .pivot(\"site\")                  # one column per site (desktop / mobile)\n",
    "    .sum(\"requests\")\n",
    "    .orderBy(\"date\", \"hour\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s11",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Window Functions\n",
    "\n",
    "Window functions compute a result for each row based on a *window* of related rows - without collapsing the DataFrame like `groupBy` does.  \n",
    "A `WindowSpec` defines the partition (equivalent to `PARTITION BY`) and the ordering within each partition.  \n",
    "Common use cases: ranking, running totals, moving averages, and comparing each row to its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s11-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the healthcare dataset - daily measurements per patient\n",
    "from pyspark.sql import Window\n",
    "\n",
    "healthcare_df = spark.read.parquet(\"data/input/health_profile_data.snappy.parquet\")\n",
    "healthcare_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s11-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window: partition by patient, order by date\n",
    "window_by_date = Window.partitionBy(\"_id\").orderBy(\"dte\")\n",
    "\n",
    "# row_number() - unique sequential number per partition (no ties)\n",
    "# rank()       - same rank for ties, then skips numbers\n",
    "# dense_rank() - same rank for ties, no gaps\n",
    "(\n",
    "    healthcare_df\n",
    "    .withColumn(\"row_num\",    F.row_number().over(window_by_date))\n",
    "    .withColumn(\"rank\",       F.rank().over(window_by_date))\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(window_by_date))\n",
    "    .select(\"_id\", \"dte\", \"resting_heartrate\", \"row_num\", \"rank\", \"dense_rank\")\n",
    ").show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s11-lag-lead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag()  - access a previous row's value (offset rows back)\n",
    "# lead() - access a future row's value  (offset rows ahead)\n",
    "# Useful for computing deltas between consecutive measurements\n",
    "(\n",
    "    healthcare_df\n",
    "    .withColumn(\"prev_hr\",  F.lag(\"resting_heartrate\", 1, 0).over(window_by_date))\n",
    "    .withColumn(\"next_hr\",  F.lead(\"resting_heartrate\", 1, 0).over(window_by_date))\n",
    "    .withColumn(\"delta_hr\", F.expr(\"resting_heartrate - prev_hr\"))\n",
    "    .select(\"_id\", \"dte\", \"resting_heartrate\", \"prev_hr\", \"next_hr\", \"delta_hr\")\n",
    ").show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s11-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window frames using rowsBetween()\n",
    "# Window.unboundedPreceding = from the start of the partition\n",
    "# Window.currentRow         = up to and including the current row\n",
    "\n",
    "window_cumulative  = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "window_last_7_days = Window.partitionBy(\"_id\").orderBy(\"dte\").rowsBetween(-6, Window.currentRow)\n",
    "\n",
    "(\n",
    "    healthcare_df\n",
    "    .withColumn(\"cumulative_avg_hr\",    F.avg(\"resting_heartrate\").over(window_cumulative))\n",
    "    .withColumn(\"rolling_7day_avg\",     F.avg(\"resting_heartrate\").over(window_last_7_days))\n",
    "    .withColumn(\"rolling_7day_max_bmi\", F.max(\"BMI\").over(window_last_7_days))\n",
    "    .select(\"_id\", \"dte\", \"resting_heartrate\", \"cumulative_avg_hr\", \"rolling_7day_avg\", \"rolling_7day_max_bmi\")\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s12",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Joins\n",
    "\n",
    "Spark supports all standard SQL join types: `inner`, `left`, `right`, `outer`, `cross`.  \n",
    "When two DataFrames share a column name after a join, Spark keeps both - use `.drop()` or alias one side before joining.  \n",
    "A *broadcast join* avoids an expensive shuffle by sending the smaller DataFrame to every executor - use when one side is small (< 10 MB by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s12-outer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join: all users, flagging which ones converted (made a purchase)\n",
    "converted_users_df = (\n",
    "    sales_df\n",
    "    .select(\"email\")\n",
    "    .distinct()\n",
    "    .withColumn(\"converted\", F.lit(True))\n",
    ")\n",
    "\n",
    "conversions_df = (\n",
    "    users_df\n",
    "    .join(converted_users_df, \"email\", \"outer\")\n",
    "    .filter(F.col(\"email\").isNotNull())\n",
    "    .na.fill(False)                    # fill null booleans → False\n",
    ")\n",
    "\n",
    "conversions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s12-left",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join: attach cart history (may not exist for all users)\n",
    "carts_df = (\n",
    "    events_df\n",
    "    .withColumn(\"items\", F.explode(\"items\"))\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(F.collect_set(\"items.item_id\").alias(\"cart\"))\n",
    ")\n",
    "\n",
    "email_carts_df = conversions_df.join(carts_df, \"user_id\", \"left\")\n",
    "email_carts_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s12-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast join - forces the small DataFrame to be broadcast to each executor\n",
    "# Avoids a full shuffle of the large DataFrame\n",
    "\n",
    "# Check the auto-broadcast threshold (default: 10 MB)\n",
    "print(\"autoBroadcastJoinThreshold:\", spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
    "\n",
    "# Small lookup: event name → event type\n",
    "event_type_df = (\n",
    "    events_df.select(\"event_name\").distinct()\n",
    "    .withColumn(\n",
    "        \"event_type\",\n",
    "        F.when(F.col(\"event_name\").isin(\"register\", \"login\"), \"initial\")\n",
    "         .when(F.col(\"event_name\").isin(\"checkout\", \"cart\", \"finalize\"), \"purchase\")\n",
    "         .otherwise(\"other\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Explicit broadcast hint on the small DataFrame\n",
    "events_enriched_df = events_df.join(\n",
    "    event_type_df.hint(\"broadcast\"),\n",
    "    \"event_name\"\n",
    ")\n",
    "\n",
    "events_enriched_df.select(\"event_name\", \"event_type\", \"device\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-query-plan",
   "metadata": {},
   "source": [
    "### Reading the Query Plan\n",
    "\n",
    "`explain(True)` shows four plan levels: Parsed, Analyzed, Optimized, and Physical.  \n",
    "In practice you care most about the **Physical plan** - look for `BroadcastHashJoin` vs `SortMergeJoin`, pushed filters (`PushedFilters` in the scan node), and which columns are actually read (`ReadSchema`).  \n",
    "This is how you verify the optimizer is doing what you expect before the job runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-explain-good",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the physical plan for the broadcast join we just built\n",
    "events_enriched_df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-explain-bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: without the broadcast hint, Spark may choose SortMergeJoin\n",
    "# - more expensive for small dimension tables\n",
    "# On your laptop Spark may auto-broadcast small tables via AQE.\n",
    "# On a cluster with large tables the difference matters.\n",
    "events_unoptimized = (\n",
    "    events_df.select(\"*\")               # unnecessary: reads all columns even unused ones\n",
    "    .join(event_type_df, \"event_name\")  # no broadcast hint on the small lookup table\n",
    "    .select(\"event_name\", \"event_type\", \"device\")\n",
    ")\n",
    "\n",
    "events_unoptimized.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-s13",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. User-Defined Functions (UDFs)\n",
    "\n",
    "UDFs let you apply arbitrary Python logic to DataFrame columns.  \n",
    "**Caveat:** UDFs are executed row-by-row in the Python interpreter, bypassing Spark's Catalyst optimizer - they are significantly slower than built-in functions.  \n",
    "Always prefer built-in `pyspark.sql.functions` over UDFs; use UDFs only when no built-in equivalent exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s13-simple-udf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: define a regular Python function\n",
    "def first_letter(email):\n",
    "    return email[0]\n",
    "\n",
    "# Step 2: wrap it as a Spark UDF (default return type is StringType)\n",
    "first_letter_udf = F.udf(first_letter)\n",
    "\n",
    "# Step 3: apply it in a select or withColumn\n",
    "sales_df.select(first_letter_udf(F.col(\"email\")).alias(\"first_letter\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s13-geo-udf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine distance UDF - computes great-circle distance in km\n",
    "# Return type must be declared explicitly for non-string types\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0                         # Earth radius in km\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = (math.sin(dlat / 2) ** 2\n",
    "         + math.cos(math.radians(lat1))\n",
    "         * math.cos(math.radians(lat2))\n",
    "         * math.sin(dlon / 2) ** 2)\n",
    "    return R * 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "geo_distance_udf = F.udf(haversine_km, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-s13-geo-apply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries dataset: ISO code, lat, lon, name\n",
    "countries_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/input/countries.csv\")\n",
    ")\n",
    "\n",
    "# Cross join Estonia against all other countries and compute distance\n",
    "distance_df = (\n",
    "    countries_df.filter(F.col(\"country\") == \"EE\")\n",
    "    .join(\n",
    "        countries_df\n",
    "        .toDF(\"join_country\", \"join_latitude\", \"join_longitude\", \"join_name\")\n",
    "        .na.drop()\n",
    "    )  # no key = cross join\n",
    "    .withColumn(\n",
    "        \"distance_km\",\n",
    "        geo_distance_udf(\"latitude\", \"longitude\", \"join_latitude\", \"join_longitude\")\n",
    "    )\n",
    "    .select(\"join_name\", \"distance_km\")\n",
    "    .orderBy(\"distance_km\")\n",
    ")\n",
    "\n",
    "distance_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-exercises",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Exercises\n",
    "\n",
    "Work through the exercises below. Each exercise builds on datasets and patterns from this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-ex1",
   "metadata": {},
   "source": [
    "### Exercise 1 - Diagnose and Fix This Pipeline\n",
    "\n",
    "The pipeline below works but has performance problems. Your task:\n",
    "\n",
    "1. Run the pipeline and call `.explain(True)` on the final result\n",
    "2. Identify at least two problems in the query plan\n",
    "3. Fix each problem and verify with `.explain(True)` that the plan improved\n",
    "4. Write a brief comment above each fix explaining what you changed and why\n",
    "\n",
    "Hints: think about join strategy, filter placement, and column selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ex1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Intentionally suboptimal pipeline - diagnose and fix ---\n",
    "\n",
    "# Step 1: read events and select all columns\n",
    "events_raw = spark.read.option(\"inferSchema\", True).json(\"data/input/events-500k.json\")\n",
    "\n",
    "events_all_columns = events_raw.select(\"*\")\n",
    "\n",
    "# Step 2: join with users - users table is small (~200k rows)\n",
    "users_all = spark.read.parquet(\"data/input/users.parquet\")\n",
    "\n",
    "joined = events_all_columns.join(users_all, events_all_columns.user_id == users_all.user_id)\n",
    "\n",
    "# Step 3: filter AFTER the join\n",
    "result = (\n",
    "    joined\n",
    "    .filter(F.col(\"event_name\") == \"finalize\")\n",
    "    .filter(F.col(\"ecommerce.total_item_quantity\") > 0)\n",
    "    .select(\"email\", \"event_name\", \"ecommerce.purchase_revenue_in_usd\")\n",
    "    .groupBy(\"email\")\n",
    "    .agg(F.sum(\"purchase_revenue_in_usd\").alias(\"total_revenue\"))\n",
    "    .orderBy(F.desc(\"total_revenue\"))\n",
    ")\n",
    "\n",
    "result.explain(True)\n",
    "result.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-ex2",
   "metadata": {},
   "source": [
    "### Exercise 2 - Choose a Partition Key\n",
    "\n",
    "You have the airlines dataset as a flat Parquet file. Three teams need to query it:\n",
    "\n",
    "- **Team A** runs a daily dashboard filtered by `airport_code` showing monthly trends\n",
    "- **Team B** runs monthly reports aggregating all airports for a given `year` and `month`\n",
    "- **Team C** occasionally looks up a specific `airport_code + year + month` combination\n",
    "\n",
    "You can only choose **one** partition layout.\n",
    "\n",
    "1. Write the table with your chosen `.partitionBy(...)` key(s)\n",
    "2. For each team, write a read query with a filter and call `.explain(True)`\n",
    "3. Check which queries get `PartitionFilters` (partition pruning) and which don’t\n",
    "4. In a comment, explain your choice: which team benefits, which team doesn’t, and why you made that tradeoff\n",
    "\n",
    "There is no single right answer - the point is to justify your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ex2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use airlines_flat_df from Section 7 - write it with your chosen partition key(s)\n",
    "# airlines_flat_df.write.mode(\"overwrite\").partitionBy(???).parquet(\"data/output/airlines_exercise\")\n",
    "\n",
    "# Team A query: filtered by airport_code, all months\n",
    "\n",
    "# Team B query: filtered by year and month, all airports\n",
    "\n",
    "# Team C query: filtered by airport_code + year + month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-further",
   "metadata": {},
   "source": [
    "---\n",
    "## Further Reading\n",
    "\n",
    "- [PySpark SQL Functions API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [Spark SQL Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "- [Window Functions in PySpark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html)\n",
    "- [UDF performance deep-dive](https://medium.com/quantumblack/spark-udf-deep-insights-in-performance-f0a95a4d8c62)\n",
    "- [Understanding Spark Query Plans](https://spark.apache.org/docs/latest/sql-performance-tuning.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
